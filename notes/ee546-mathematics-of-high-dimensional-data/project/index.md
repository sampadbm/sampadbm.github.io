---
themes: ["muted","colorful"]
---

# EE546-Mathematics of High Dimensional Data
<p style="text-align:center; color:#7A306C"> <b>25th October, 2022</b> </p>

<p style='text-align:center;color:green'><b> 
Project EE546 Fall2022 | Prof. Mahdi Soltanolkotabi
</b></p>

---


## Abstract

A tensor of order 3 can represent variety of data like images and spatiotemporal data. In this project we compare performance of popular imputation methods on an image and a traffic dataset. Specifically, we are interested in finding how tensor decomposition based imputation methods compare to methods like mean imputation, matrix decomposition based methods and neural networks.


## Problem formulation

We are given an order 3 tensor $T \in \mathbb{R}^{m \times n \times p}$ that is a representation of data generated by some data generating process. A binary mask $\Omega \in \mathbb{\{0,1\}}^{m \times n \times p}$ provides location of unobserved entries, i.e if $\Omega[i,j,k]=0$, then $A[i,j,k]$ is unobserved. The task is to best estimate the unobserved value having seen only the observed value.

We also get matrix representations of the observations and the mask by unfolding the tensors along one of the modes

## Models

We compare the following techniques - 

| Method | Description |
|---|---|
| Mean imputation | Mean value of the column |
| Tensor decompostition | Low rank decomposition followed by reconstruction |
| Matrix imputation | Low rank decomposition followed by reconstruction |
| Neural net | input = position, output = value at position -> train on observed positions |


#### 1. Mean Imputation
This is implemented using SimpleImputer from sklearn library. The values are mean of the columns using the matrix representation. We compare the various unfolding modes of the tensor for the matrix representation.


$$
	M[i,j] = \frac{ \sum_{i} \mathbf{M}[i,j] \mathbf{\Omega}[i,j] }{\sum_i \mathbf{\Omega}[i,j]}
$$
#### 2. Tensor Decomposition
The tensor representation of the observed data is assumed to be low rank.  We use CP decomposition which is carried out using alternating least squares method. Reconstructing the tensor using the factors gives us the unobserved values.

Decomposition using ALS
$$
	\underset{\mathbf{a_i,b_i,c_i}}{\text{minimize }}{|| \mathbf{\Omega} \odot \big(\mathbf{T} - \sum_{i}^{r} \mathbf{a_i} \otimes \mathbf{b_i} \otimes \mathbf{c_i} \big) ||_F}
$$ 

Reconstruction 

$$
	\mathbf{\hat{T}} = \sum_{i}^{r} \mathbf{a_i} \otimes \mathbf{b_i} \otimes \mathbf{c_i}
$$

#### 3. Matrix Decomposition
Same as tensor decomposition but on the matrix representation of the observations. 

Decomposition using ALS
$$
	\underset{\mathbf{a_i, b_i}}{\text{minimize }}{|| M - \sum_{i=1}^{r}{\mathbf{a_i b_i^T}}||_F}
$$

Reconstruction 

$$
	\mathbf{\hat{T}} = \sum_{i}^{r} \mathbf{a_i} \mathbf{b_i^T}

$$

#### 4. Neural Net
This method is tried on the matrix and the tensor representations. For the tensor representation, the network is a function $f : \mathbb{R}^{3} \rightarrow \mathbb{R}$ and $g: \mathbb{R}^2 \rightarrow \mathbb{R}$ for the matrix representation. The network is trained on the observed indices and the unobserved entries are predictions of the network on the unobserved indices.

$$
	\underset{\theta}{\text{minimize }}{\sum_{ \substack{i,j,k \\\\ \Omega[i,j,k]=1}} \bigg( f(\theta, [i,j,k]) - \mathbf{T}[i,j,k] \bigg)^2}
$$

Reconstruction 

$$
	\mathbf{\hat{T}}[i,j,k] = f(\boldsymbol{ \theta^*}, [i,j,k])
$$

## Evaluation

We evaluate the performance of the various models using the reconstruction loss $L_R$ and imputation loss $L_I$.

$$
	L_R = \frac{|| \mathbf{\Omega} \circ (\mathbf{\hat{T} - T})||}{||\mathbf{T}||}
$$

$$
	L_I = \frac{|| (\mathbf{1 - \Omega}) \circ (\mathbf{\hat{T} - T})||}{||\mathbf{T}||}
$$

